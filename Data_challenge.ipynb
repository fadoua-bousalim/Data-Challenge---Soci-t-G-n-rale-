{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import fancyimpute as fancy\n",
    "import warnings\n",
    "import keras\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Missing_values(data):\n",
    "    total = data.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total,percent], axis=1, keys=['Total', 'Pourcentage'])\n",
    "    #Affiche que les variables avec des na\n",
    "    print(missing_data[(percent>0)],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_excel(\"Train.xlsx\")\n",
    "test = pd.read_excel(\"Test.xlsx\")\n",
    "train_out = pd.read_csv(\"train_output.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.month = pd.Categorical(train.month)\n",
    "test.month = pd.Categorical(test.month)\n",
    "\n",
    "train.country = pd.Categorical(train.country)\n",
    "test.country = pd.Categorical(test.country)\n",
    "\n",
    "train = train.set_index(\"ID\")\n",
    "train.index.name = None\n",
    "\n",
    "test = test.set_index(\"ID\")\n",
    "test.index.name = None\n",
    "\n",
    "train_out = train_out.set_index(\"ID\")\n",
    "train_out.index.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_with_target = train.copy()\n",
    "train_with_target['Target'] = train_out['Target'].copy()\n",
    "corr = train_with_target.corr()\n",
    "z = corr[abs(corr['Target']) < 0.01]\n",
    "var_inut = z.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filter = train.drop(var_inut,axis = 1)\n",
    "test_filter = test.drop(var_inut,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/10159 with 0 missing, elapsed time: 60.605\n",
      "Imputing row 101/10159 with 0 missing, elapsed time: 60.637\n",
      "Imputing row 201/10159 with 0 missing, elapsed time: 60.637\n",
      "Imputing row 301/10159 with 0 missing, elapsed time: 60.638\n",
      "Imputing row 401/10159 with 0 missing, elapsed time: 60.640\n",
      "Imputing row 501/10159 with 0 missing, elapsed time: 60.642\n",
      "Imputing row 601/10159 with 0 missing, elapsed time: 60.643\n",
      "Imputing row 701/10159 with 0 missing, elapsed time: 60.646\n",
      "Imputing row 801/10159 with 0 missing, elapsed time: 60.646\n",
      "Imputing row 901/10159 with 0 missing, elapsed time: 60.647\n",
      "Imputing row 1001/10159 with 0 missing, elapsed time: 60.648\n",
      "Imputing row 1101/10159 with 0 missing, elapsed time: 60.650\n",
      "Imputing row 1201/10159 with 0 missing, elapsed time: 60.656\n",
      "Imputing row 1301/10159 with 0 missing, elapsed time: 60.659\n",
      "Imputing row 1401/10159 with 0 missing, elapsed time: 60.661\n",
      "Imputing row 1501/10159 with 0 missing, elapsed time: 60.667\n",
      "Imputing row 1601/10159 with 0 missing, elapsed time: 60.668\n",
      "Imputing row 1701/10159 with 0 missing, elapsed time: 60.675\n",
      "Imputing row 1801/10159 with 0 missing, elapsed time: 60.678\n",
      "Imputing row 1901/10159 with 0 missing, elapsed time: 60.681\n",
      "Imputing row 2001/10159 with 0 missing, elapsed time: 60.684\n",
      "Imputing row 2101/10159 with 0 missing, elapsed time: 60.687\n",
      "Imputing row 2201/10159 with 0 missing, elapsed time: 60.690\n",
      "Imputing row 2301/10159 with 0 missing, elapsed time: 60.693\n",
      "Imputing row 2401/10159 with 0 missing, elapsed time: 60.694\n",
      "Imputing row 2501/10159 with 0 missing, elapsed time: 60.695\n",
      "Imputing row 2601/10159 with 0 missing, elapsed time: 60.696\n",
      "Imputing row 2701/10159 with 0 missing, elapsed time: 60.700\n",
      "Imputing row 2801/10159 with 0 missing, elapsed time: 60.702\n",
      "Imputing row 2901/10159 with 0 missing, elapsed time: 60.703\n",
      "Imputing row 3001/10159 with 0 missing, elapsed time: 60.704\n",
      "Imputing row 3101/10159 with 0 missing, elapsed time: 60.708\n",
      "Imputing row 3201/10159 with 0 missing, elapsed time: 60.709\n",
      "Imputing row 3301/10159 with 0 missing, elapsed time: 60.711\n",
      "Imputing row 3401/10159 with 0 missing, elapsed time: 60.713\n",
      "Imputing row 3501/10159 with 0 missing, elapsed time: 60.713\n",
      "Imputing row 3601/10159 with 0 missing, elapsed time: 60.714\n",
      "Imputing row 3701/10159 with 0 missing, elapsed time: 60.714\n",
      "Imputing row 3801/10159 with 0 missing, elapsed time: 60.715\n",
      "Imputing row 3901/10159 with 0 missing, elapsed time: 60.716\n",
      "Imputing row 4001/10159 with 0 missing, elapsed time: 60.716\n",
      "Imputing row 4101/10159 with 0 missing, elapsed time: 60.718\n",
      "Imputing row 4201/10159 with 0 missing, elapsed time: 60.718\n",
      "Imputing row 4301/10159 with 0 missing, elapsed time: 60.719\n",
      "Imputing row 4401/10159 with 0 missing, elapsed time: 60.720\n",
      "Imputing row 4501/10159 with 0 missing, elapsed time: 60.721\n",
      "Imputing row 4601/10159 with 0 missing, elapsed time: 60.722\n",
      "Imputing row 4701/10159 with 1 missing, elapsed time: 60.723\n",
      "Imputing row 4801/10159 with 0 missing, elapsed time: 60.725\n",
      "Imputing row 4901/10159 with 0 missing, elapsed time: 60.726\n",
      "Imputing row 5001/10159 with 0 missing, elapsed time: 60.727\n",
      "Imputing row 5101/10159 with 0 missing, elapsed time: 60.728\n",
      "Imputing row 5201/10159 with 0 missing, elapsed time: 60.730\n",
      "Imputing row 5301/10159 with 0 missing, elapsed time: 60.731\n",
      "Imputing row 5401/10159 with 0 missing, elapsed time: 60.731\n",
      "Imputing row 5501/10159 with 0 missing, elapsed time: 60.731\n",
      "Imputing row 5601/10159 with 13 missing, elapsed time: 60.732\n",
      "Imputing row 5701/10159 with 0 missing, elapsed time: 60.734\n",
      "Imputing row 5801/10159 with 0 missing, elapsed time: 60.735\n",
      "Imputing row 5901/10159 with 0 missing, elapsed time: 60.736\n",
      "Imputing row 6001/10159 with 0 missing, elapsed time: 60.737\n",
      "Imputing row 6101/10159 with 0 missing, elapsed time: 60.739\n",
      "Imputing row 6201/10159 with 0 missing, elapsed time: 60.740\n",
      "Imputing row 6301/10159 with 0 missing, elapsed time: 60.741\n",
      "Imputing row 6401/10159 with 0 missing, elapsed time: 60.743\n",
      "Imputing row 6501/10159 with 0 missing, elapsed time: 60.744\n",
      "Imputing row 6601/10159 with 0 missing, elapsed time: 60.745\n",
      "Imputing row 6701/10159 with 0 missing, elapsed time: 60.746\n",
      "Imputing row 6801/10159 with 0 missing, elapsed time: 60.747\n",
      "Imputing row 6901/10159 with 0 missing, elapsed time: 60.748\n",
      "Imputing row 7001/10159 with 0 missing, elapsed time: 60.749\n",
      "Imputing row 7101/10159 with 0 missing, elapsed time: 60.750\n",
      "Imputing row 7201/10159 with 0 missing, elapsed time: 60.751\n",
      "Imputing row 7301/10159 with 0 missing, elapsed time: 60.754\n",
      "Imputing row 7401/10159 with 0 missing, elapsed time: 60.754\n",
      "Imputing row 7501/10159 with 0 missing, elapsed time: 60.755\n",
      "Imputing row 7601/10159 with 0 missing, elapsed time: 60.756\n",
      "Imputing row 7701/10159 with 0 missing, elapsed time: 60.757\n",
      "Imputing row 7801/10159 with 0 missing, elapsed time: 60.758\n",
      "Imputing row 7901/10159 with 0 missing, elapsed time: 60.760\n",
      "Imputing row 8001/10159 with 0 missing, elapsed time: 60.761\n",
      "Imputing row 8101/10159 with 0 missing, elapsed time: 60.761\n",
      "Imputing row 8201/10159 with 0 missing, elapsed time: 60.763\n",
      "Imputing row 8301/10159 with 0 missing, elapsed time: 60.764\n",
      "Imputing row 8401/10159 with 0 missing, elapsed time: 60.765\n",
      "Imputing row 8501/10159 with 0 missing, elapsed time: 60.766\n",
      "Imputing row 8601/10159 with 0 missing, elapsed time: 60.767\n",
      "Imputing row 8701/10159 with 0 missing, elapsed time: 60.768\n",
      "Imputing row 8801/10159 with 0 missing, elapsed time: 60.769\n",
      "Imputing row 8901/10159 with 0 missing, elapsed time: 60.769\n",
      "Imputing row 9001/10159 with 0 missing, elapsed time: 60.771\n",
      "Imputing row 9101/10159 with 0 missing, elapsed time: 60.773\n",
      "Imputing row 9201/10159 with 0 missing, elapsed time: 60.775\n",
      "Imputing row 9301/10159 with 0 missing, elapsed time: 60.776\n",
      "Imputing row 9401/10159 with 0 missing, elapsed time: 60.777\n",
      "Imputing row 9501/10159 with 0 missing, elapsed time: 60.778\n",
      "Imputing row 9601/10159 with 0 missing, elapsed time: 60.779\n",
      "Imputing row 9701/10159 with 0 missing, elapsed time: 60.779\n",
      "Imputing row 9801/10159 with 0 missing, elapsed time: 60.779\n",
      "Imputing row 9901/10159 with 0 missing, elapsed time: 60.780\n",
      "Imputing row 10001/10159 with 0 missing, elapsed time: 60.781\n",
      "Imputing row 10101/10159 with 0 missing, elapsed time: 60.782\n"
     ]
    }
   ],
   "source": [
    "train_filled = fancy.KNN(k=10).complete(train_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_filled = pd.DataFrame(train_filled)\n",
    "df_train_filled.columns = train_filter.columns\n",
    "df_train_filled['month'] = df_train_filled['month'].astype('category')\n",
    "df_train_filled['country'] = df_train_filled['country'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_expand = pd.get_dummies(df_train_filled)\n",
    "test_expand = pd.get_dummies(test_filter)\n",
    "train_out.index = train_expand.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_out.index = train_expand.index\n",
    "test_expand.columns = train_expand.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost et GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=5)]: Done 108 out of 108 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.7,\n",
       "  'learning_rate': 0.03,\n",
       "  'max_depth': 6,\n",
       "  'n_estimators': 500,\n",
       "  'nthread': 4},\n",
       " 0.8121641334664079)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'learning_rate': [0.03,0.05,0.1], #so called `eta` value\n",
    "              'max_depth': [6,7,8,10],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [5,100,500]}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "                   cv=3, \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "X = train_expand\n",
    "y = train_out\n",
    "\n",
    "clf.fit(X,y)\n",
    "\n",
    "clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "Keras_reg = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_expand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d7cace0eb9a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_expand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_expand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_expand' is not defined"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "X_train = train_expand.copy()\n",
    "y_train = train_out.copy()\n",
    "X_test = test_expand.copy()\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "\n",
    "X_train = X_train.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "X_test = X_test.as_matrix()\n",
    "\n",
    "models = [RandomForestRegressor(n_jobs = -1),\n",
    "          ExtraTreesRegressor(n_jobs = -1),\n",
    "           XGBRegressor(colsample_bytree= 0.7,learning_rate= 0.03, max_depth= 6, n_estimators= 500,nthread= 4)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:   [regression]\n",
      "metric: [roc_auc_score]\n",
      "\n",
      "model 0: [RandomForestRegressor]\n",
      "    fold 0: [0.75803361]\n",
      "    fold 1: [0.76299407]\n",
      "    fold 2: [0.78033499]\n",
      "    fold 3: [0.78392433]\n",
      "    ----\n",
      "    MEAN:   [0.77125819]\n",
      "\n",
      "model 1: [BaggingRegressor]\n",
      "    fold 0: [0.77501552]\n",
      "    fold 1: [0.77660006]\n",
      "    fold 2: [0.77705889]\n",
      "    fold 3: [0.77898915]\n",
      "    ----\n",
      "    MEAN:   [0.77704058]\n",
      "\n",
      "model 2: [XGBRegressor]\n",
      "    fold 0: [0.80981292]\n",
      "    fold 1: [0.80880142]\n",
      "    fold 2: [0.82178481]\n",
      "    fold 3: [0.81493769]\n",
      "    ----\n",
      "    MEAN:   [0.81353325]\n",
      "\n",
      "model 3: [XGBRegressor]\n",
      "    fold 0: [0.80981292]\n",
      "    fold 1: [0.80880142]\n",
      "    fold 2: [0.82178481]\n",
      "    fold 3: [0.81493769]\n",
      "    ----\n",
      "    MEAN:   [0.81353325]\n",
      "\n",
      "model 4: [XGBRegressor]\n",
      "    fold 0: [0.80981292]\n",
      "    fold 1: [0.80880142]\n",
      "    fold 2: [0.82178481]\n",
      "    fold 3: [0.81493769]\n",
      "    ----\n",
      "    MEAN:   [0.81353325]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute stacking features \n",
    "S_train, S_test = stacking(models, X_train, y_train, X_test, \n",
    "    regression = True, metric = roc_auc_score, n_folds = 4, \n",
    "    shuffle = True, random_state = 0, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize 2-nd level model\n",
    "model = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.7,\n",
       "  'learning_rate': 0.03,\n",
       "  'max_depth': 6,\n",
       "  'n_estimators': 100,\n",
       "  'nthread': 4},\n",
       " 0.80844022373438795)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'learning_rate': [0.03,0.05,0.1], #so called `eta` value\n",
    "              'max_depth': [6,7,8,10],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [100,500,1000,2000]}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(model, parameters, n_jobs=-1, \n",
    "                   cv=5, \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "X = S_train\n",
    "y = y_train\n",
    "\n",
    "clf.fit(X,y)\n",
    "\n",
    "clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = clf.predict(S_test)\n",
    "sub = pd.DataFrame({\"ID\":test_expand.index, \"Target\": y_test},columns=[\"ID\",\"Target\"])\n",
    "sub.to_csv(\"sub_2.csv\",index=False,sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
